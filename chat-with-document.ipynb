{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70dafb23",
   "metadata": {},
   "source": [
    "# Zania | AI Challenge\n",
    "\n",
    "## Problem Statement\n",
    "Create an AI agent that leverages the capabilities of a large language model. This agent should be able to extract answers based on the content of a large PDF document and post the results on Slack. Ideally, you use OpenAI LLMs. You can also use the Langchain or LLama Index framework to implement this agentic functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077c2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5743c4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "807a7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9b1ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ.get('SLACK_USER_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08954b6",
   "metadata": {},
   "source": [
    "## Load pdf file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345e7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Loaders.\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "    \n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!!')\n",
    "        return None\n",
    "    \n",
    "    data = loader.load()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335425d6",
   "metadata": {},
   "source": [
    "###  Chunking\n",
    "- `Chunking is the process of breaking down large pieces of text into smaller segments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876232c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size= chunk_size, chunk_overlap= 0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0afdcd",
   "metadata": {},
   "source": [
    "### Text embedding\n",
    "- We'll be using Openai's text embedding ADA 002, which has a cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc827831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total token: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 100 * 0.0004:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20f3e9",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c504b6b",
   "metadata": {},
   "source": [
    "## Using Chroma as a Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f619d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991754fd",
   "metadata": {},
   "source": [
    "- Now defining a new function that creates the embeddings.\n",
    "\n",
    "- Using the OpenAI embeddings class, saves them in a database and returns the database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e56885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_chroma(chunks, persist_dir='./chroma.db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model= 'text-embedding-3-small',\n",
    "        dimensions = 1536\n",
    "    )    \n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_dir)\n",
    "    return vector_store\n",
    "    \n",
    "\n",
    "def load_embedding_chroma(persist_dir='./chroma.db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model= 'text-embedding-3-small',\n",
    "        dimensions = 1536\n",
    "    )    \n",
    "    vector_store = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b6786c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading handbook.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('handbook.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embedding_chroma(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68608032",
   "metadata": {},
   "source": [
    "### Check the cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "135e0f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token: 27505\n",
      "Embedding Cost in USD: 0.110020\n"
     ]
    }
   ],
   "source": [
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16a26b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "vector_store = load_embedding_chroma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffc3e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ff246",
   "metadata": {},
   "source": [
    "## Using a Custom prompt\n",
    "- Let's explore how to change the system prompt and use prompt engineering techniques to ask questions in specific ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d237f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "\n",
    "memory =ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "system_template = r'''\n",
    "Use the following pieces of context to answer the user's question.\n",
    "If you don't know the answer in the provided context, just respond \"Data Not Available\"\n",
    "-------------\n",
    "Context:```{context}```\n",
    "'''\n",
    "\n",
    "user_template = '''\n",
    "Question: ```{question}```\n",
    "Chat History: ```{chat_history}```\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Chain type stuff means use all of the text from the documents.\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type='stuff',\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bffbd387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['chat_history', 'context', 'question'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='\\nUse the following pieces of context to answer the user\\'s question.\\nIf you don\\'t know the answer in the provided context, just respond \"Data Not Available\"\\n-------------\\nContext:```{context}```\\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='\\nQuestion: ```{question}```\\nChat History: ```{chat_history}```\\n'))]\n"
     ]
    }
   ],
   "source": [
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8747ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "537ddab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# db = load_embedding_chroma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5d6fe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zania, Inc.\n"
     ]
    }
   ],
   "source": [
    "q = \"What is the name of the company?\"\n",
    "result = ask_question(q, crc)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "440eba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"answer\": \"Shruti Gupta\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "q = \"Who is the CEO of the company?\"\n",
    "result = ask_question(q, crc)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb96ba2",
   "metadata": {},
   "source": [
    "### Here we can ask multiple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e8a9994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While Quit or Exit to quit.\n",
      "Question #1: What is the name of the company?\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"Zania, Inc.\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "Question #2: Who is the CEO of the company?\n",
      "\n",
      "Answer: ```json\n",
      "{\n",
      "    \"answer\": \"Shruti Gupta\"\n",
      "}\n",
      "```\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "Question #3: What is their vacation policy?\n",
      "\n",
      "Answer: ```json\n",
      "{\n",
      "    \"answer\": \"PTO may be used for vacation, sick time, or other personal matters.\"\n",
      "}\n",
      "```\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "Question #4: What is the termination policy?\n",
      "\n",
      "Answer: ```json\n",
      "{\n",
      "    \"answer\": \"Violation of attendance policy or job abandonment may result in disciplinary action, up to and including termination of employment. Violation of other policies may also lead to termination of employment.\"\n",
      "}\n",
      "```\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "Question #5: exit\n",
      "Quitting ... bye bye\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('While Quit or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i += 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting ... bye bye')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "        \n",
    "    result = ask_question(q, crc)\n",
    "    answer = result['answer']\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n{\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2765b1b0",
   "metadata": {},
   "source": [
    "Thank you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269da309",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa501b7",
   "metadata": {},
   "source": [
    "### With slack integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0ae50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet  slack_sdk > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b847766e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SlackGetChannel(client=<slack_sdk.web.client.WebClient object at 0x11e753a50>),\n",
       " SlackGetMessage(client=<slack_sdk.web.client.WebClient object at 0x11e7508d0>),\n",
       " SlackScheduleMessage(client=<slack_sdk.web.client.WebClient object at 0x11ead6450>),\n",
       " SlackSendMessage(client=<slack_sdk.web.client.WebClient object at 0x11ead7910>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits import SlackToolkit\n",
    "toolkit = SlackToolkit()\n",
    "tools = toolkit.get_tools()\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f25e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ef20053",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\")\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "agent = create_openai_tools_agent(\n",
    "    tools=toolkit.get_tools(),\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e59bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_send_to_slack(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    json = {\n",
    "        'question': result['question'],\n",
    "        'answer': result['answer']\n",
    "    }\n",
    "    agent_executor.invoke({\n",
    "        \"input\": f\"Send {json} in the #gen-ai channel. Note use `channel` as key of channel id, and `message` as key of content to sent in the channel.\"\n",
    "    })\n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8bf9f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the name of the company?', 'answer': 'Zania, Inc.'}\n"
     ]
    }
   ],
   "source": [
    "q = \"What is the name of the company?\"\n",
    "result = ask_question_send_to_slack(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683efb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
